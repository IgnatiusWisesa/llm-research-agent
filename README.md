# ğŸ¤– LLM Research Agent (CLI)

A command-line research assistant that takes a natural language question, decomposes it into search queries, fetches real-time web results, reflects on coverage, and synthesizes a short, cited answer â€” powered by **Gemini** and **Google Custom Search**.

![Python](https://img.shields.io/badge/python-3.11-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Gemini API](https://img.shields.io/badge/Gemini-1.5--flash-yellow)
![Redis Cache](https://img.shields.io/badge/Redis-enabled-red)

---

## âœ¨ Features

- âœ… **Query Generator** â€” Breaks the question into 3â€“5 useful search queries
- ğŸ” **Web Search** â€” Uses Google CSE to retrieve real-time search results
- ğŸ§  **Reflective Check** â€” Uses Gemini to verify if all information slots are covered
- ğŸ§¾ **Synthesis Engine** â€” Answers questions with markdown-style citations
- ğŸ”„ **Citation Remapping** â€” Compresses citation IDs (e.g., `[1, 2]` instead of `[3, 7]`)
- âš¡ **Redis Caching** â€” Caches previous results for speed and repeatability
- ğŸ§ª **Unit Tests** â€” Covers normal use, timeout, no docs, retry, and 429 fallback

---

## ğŸ“ Project Structure

```
llm-research-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ agent/
â”‚       â”œâ”€â”€ nodes/
â”‚       â”‚   â”œâ”€â”€ generate_queries.py
â”‚       â”‚   â”œâ”€â”€ reflect.py
â”‚       â”‚   â””â”€â”€ synthesize.py
â”‚       â”œâ”€â”€ tools/
â”‚       â”‚   â””â”€â”€ websearch.py
â”‚       â”œâ”€â”€ types/
â”‚       â”‚   â””â”€â”€ document.py
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ slot_utils.py
â”‚           â””â”€â”€ cache_utils.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_reflect.py
â”‚   â”œâ”€â”€ test_queries.py
â”‚   â””â”€â”€ conftest.py
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Clone the Repo & Install Dependencies

```bash
git clone https://github.com/IgnatiusWisesa/llm-research-agent.git
cd llm-research-agent

pip install -r requirements.txt
cp .env.example .env
```

### 2. Configure `.env`

```
GEMINI_API_KEY=your_google_genai_key
GOOGLE_API_KEY=your_google_search_api_key
GOOGLE_CSE_ID=your_custom_search_engine_id
REDIS_HOST=localhost
```

---

## âœ… Run Unit Tests

```bash
pytest -q tests/
```

Scenarios tested:
- âœ”ï¸ Happy path
- â›” No results
- ğŸ” Two-round follow-up
- ğŸ”„ Timeout + retries
- ğŸš« HTTP 429 (rate-limited)

---

## ğŸ§  CLI Example

```bash
python src/main.py "Who won the 2022 FIFA World Cup?"
```

Output:

```json
{
  "status": "complete",
  "answer": "Argentina won the 2022 FIFA World Cup, defeating France 4-2 on penalties after a 3-3 draw. [1, 2]",
  "citations": [
    {
      "id": 1,
      "title": "2022 FIFA World Cup final - Wikipedia",
      "url": "https://en.wikipedia.org/wiki/2022_FIFA_World_Cup_final"
    },
    {
      "id": 2,
      "title": "How Argentina won the 2022 World Cup - ESPN",
      "url": "https://www.espn.com/soccer/story/_/id/123456"
    }
  ]
}
```

---

## ğŸ Python API Usage

```python
from agent.nodes.synthesize import synthesize
from agent.types.document import Document

question = "Explain black holes like I'm five."

docs = [
    Document(
        title="What Is a Black Hole?",
        snippet="A black hole is a place in space where gravity is so strong nothing can escape.",
        url="https://example.com/black-hole"
    )
]

result = synthesize(question, docs)
print(result["answer"])
```

---

## ğŸ§­ Architecture Flow

```text
    [User Question]
           â†“
  generate_queries()
           â†“
   WebSearchTool.run()
           â†“
        Documents
           â†“
         reflect()
        â†™       â†˜
   enough?      suggest new queries
     â†“
 synthesize()
     â†“
[Answer + Citations]
     â†“
     Cache (Redis)
```

---

## ğŸ“¦ Requirements

- Python 3.11+
- Redis (optional but recommended)
- Gemini API Key (via Google AI Studio)
- Google CSE API Key + Custom Search Engine ID

---

## ğŸ“„ License

MIT License  
Â© 2025 [Ignatius Wisesa](https://github.com/IgnatiusWisesa)