# ğŸ¤– LLM Research Agent (CLI + API)

A command-line research assistant that takes a natural language question, decomposes it into search queries, fetches real-time web results, reflects on coverage, and synthesizes a short, cited answer â€” powered by **Gemini** and **Google Custom Search**.

![Python](https://img.shields.io/badge/python-3.11-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Gemini API](https://img.shields.io/badge/Gemini-1.5--flash-yellow)
![Redis Cache](https://img.shields.io/badge/Redis-enabled-red)

---

## âœ¨ Features

- âœ… **Query Generator** â€” Breaks the question into 3â€“5 useful search queries
- ğŸ” **Web Search** â€” Uses Google CSE to retrieve real-time search results
- ğŸ§  **Reflective Check** â€” Uses Gemini to verify if all information slots are covered
- ğŸ§¾ **Synthesis Engine** â€” Answers questions with markdown-style citations
- ğŸ”„ **Citation Remapping** â€” Compresses citation IDs (e.g., `[1, 2]` instead of `[3, 7]`)
- âš¡ **Redis Caching** â€” Caches previous results for speed and repeatability
- ğŸ“Š **OpenTelemetry & Prometheus** â€” Collects tool call counts and latencies
- ğŸ§ª **Unit Tests** â€” Covers normal use, timeout, no docs, retry, and 429 fallback

---

## ğŸ“ Project Structure

```
llm-research-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ api_server.py
â”‚   â””â”€â”€ agent/
â”‚       â”œâ”€â”€ nodes/
â”‚       â”œâ”€â”€ tools/
â”‚       â”œâ”€â”€ types/
â”‚       â””â”€â”€ utils/
â”œâ”€â”€ tests/
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Clone the Repo & Install Dependencies

```bash
git clone https://github.com/IgnatiusWisesa/llm-research-agent.git
cd llm-research-agent

pip install -r requirements.txt
cp .env.example .env
```

### 2. Configure `.env`

```env
GEMINI_API_KEY=your_google_genai_key
GOOGLE_API_KEY=your_google_search_api_key
GOOGLE_CSE_ID=your_custom_search_engine_id
REDIS_HOST=localhost
```

---

## ğŸ§ª Run Unit Tests

```bash
pytest -q tests/
```

Scenarios tested:
- âœ”ï¸ Happy path
- â›” No results
- ğŸ” Two-round follow-up
- ğŸ”„ Timeout + retries
- ğŸš« HTTP 429 (rate-limited)

---

## ğŸ§  CLI Usage

```bash
python src/main.py "Compare Kubernetes HPA and KEDA"
```

---

## ğŸ–¥ï¸ Run the Backend API Server

```bash
uvicorn agent.api_server:app --reload --app-dir src
```

- FastAPI runs on: [http://127.0.0.1:8000](http://127.0.0.1:8000)
- Metrics endpoint: [http://127.0.0.1:8000/metrics](http://127.0.0.1:8000/metrics)
- Query endpoint: `POST /api/query` with JSON body:

```json
{
  "question": "What is the difference between Kubernetes HPA and KEDA?"
}
```

---

## ğŸ§­ Architecture Flow

```text
    [User Question]
           â†“
  generate_queries()
           â†“
   WebSearchTool.run()
           â†“
        Documents
           â†“
         reflect()
        â†™       â†˜
   enough?      suggest new queries
     â†“
 synthesize()
     â†“
[Answer + Citations]
     â†“
     Cache (Redis)
```

---

## ğŸ“¦ Requirements

- Python 3.11+
- Redis (optional but recommended)
- Gemini API Key (via Google AI Studio)
- Google CSE API Key + Custom Search Engine ID

---

## ğŸ“Š Monitoring & Metrics

This app uses `prometheus-fastapi-instrumentator` and `OpenTelemetry` to expose internal performance metrics.

- Prometheus-compatible metrics at: `GET /metrics`
- Includes:
  - GC stats
  - Tool call counts & latencies
  - HTTP request durations and status codes

---

## ğŸ“„ License

MIT License  
Â© 2025 [Ignatius Wisesa](https://github.com/IgnatiusWisesa)
